# -*- coding: utf-8 -*-
"""DSAI201_Project_KareemEzzaldin200201972.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13GYCEduX-r0JnVn7aEzBNTKQ36QJfpRx
"""

#install the Pyterrier framework
!pip install python-terrier
# install the nltk modules
!pip install nltk

import pyterrier as pt

if not pt.started():
  # In this lab, we need to specify that we start PyTerrier with PRF enabled
  pt.init(boot_packages=["com.github.terrierteam:terrier-prf:-SNAPSHOT"])

# Import Libraries

import re
import pandas as pd
import nltk
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub

from nltk.stem import *
from nltk.stem.porter import *
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

corpus_directory = '/content/corpus.jsonl'

import pandas as pd
import os

# Function to ensure directory exists
def ensure_dir(file_path):
    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)

# Function to convert JSONL to CSV
def jsonl_to_csv(jsonl_file_path, csv_file_path):
    ensure_dir(csv_file_path)
    df = pd.read_json(jsonl_file_path, lines=True)
    df.to_csv(csv_file_path, index=False)

# Function to convert TSV to CSV
def tsv_to_csv(tsv_file_path, csv_file_path):
    ensure_dir(csv_file_path)
    df = pd.read_csv(tsv_file_path, sep='\t')
    df.to_csv(csv_file_path, index=False)

# Paths for JSONL files
corpus_jsonl_path = '/content/corpus.jsonl'
queries_jsonl_path = '/content/queries.jsonl'

# Paths for TSV files
test_tsv_path = '/content/test.tsv'
train_tsv_path = '/content/train.tsv'

# Output CSV paths
corpus_csv_path = '/mnt/data/corpus.csv'
queries_csv_path = '/mnt/data/queries.csv'
test_csv_path = '/mnt/data/test.csv'
train_csv_path = '/mnt/data/train.csv'

# Convert files
jsonl_to_csv(corpus_jsonl_path, corpus_csv_path)
jsonl_to_csv(queries_jsonl_path, queries_csv_path)
tsv_to_csv(test_tsv_path, test_csv_path)
tsv_to_csv(train_tsv_path, train_csv_path)

df=pd.read_csv("/mnt/data/corpus.csv")
df

#Import the necessary modules:
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import re # used to clean the data
#to display the full text on the notebook without truncation
pd.set_option('display.max_colwidth', 150)

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
print(stopwords.words('english'))

nltk.download('punkt')

from nltk.stem import *
from nltk.stem.porter import *

# Initialize Porter stemmer
stemmer = PorterStemmer()

def Steem_text(text):

    tokens = word_tokenize(text)
    stemmed_tokens = [stemmer.stem(word) for word in tokens]
    # print (tokens)
    return ' '.join(stemmed_tokens)

# Function to remove stopwords
def remove_stopwords(text):

    tokens = word_tokenize(text)
    filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words] #Lower is used to normalize al the words make them in lower case
    print('Tokens are:',tokens,'\n')
    return ' '.join(filtered_tokens)

import pandas as pd

#the docno will be our tweetID
df["docno"]=df["_id"].astype(str)
df[["docno"]]

def clean(text):
   text = re.sub(r"http\S+", " ", text) # remove urls
   text = re.sub(r"RT ", " ", text) # remove rt
   text = re.sub(r"@[\w]*", " ", text) # remove handles
   text = re.sub(r"[\.\,\#_\|\:\?\?\/\=]", " ", text) # remove special characters
   text = re.sub(r'\t', ' ', text) # remove tabs
   text = re.sub(r'\n', ' ', text) # remove line jump
   text = re.sub(r"\s+", " ", text) # remove extra white space
   text = text.strip()
   return text

df['processed_title'] = df['title'].apply(clean)
df

df['processed_text'] = df['text'].apply(clean)
df

df['processed_text'] = df['processed_text'].apply(remove_stopwords)
df['processed_title'] = df['processed_title'].apply(remove_stopwords)

df['processed_text'] = df['processed_text'].apply(Steem_text)
df['processed_title'] = df['processed_title'].apply(Steem_text)
df

indexer = pt.DFIndexer("./myFirstIndex", overwrite=True)

# index the text, record the docnos as metadata
index_ref = indexer.index(df["processed_text"], df["processed_title"], df["docno"])

print(index_ref.toString())
#we will first load the index
index = pt.IndexFactory.of(index_ref)
#we will call getCollectionStatistics() to check the stats
print(index.getCollectionStatistics().toString())

for kv in index.getLexicon():
  print("%s -> %s " % (kv.getKey(), kv.getValue().toString()))

"""Query Processing"""

#we need to process the query also as we did for documents
def preprocess(sentence):
  sentence = remove_stopwords(sentence)
  sentence = clean(sentence)
  sentence = Steem_text(sentence)

  return sentence

query="study"
query = preprocess(query)
query

splited_query = query.split()
len(splited_query)

# Identify the documents that have the query

# Function(1) : Split the documents into tokens

def split_doc_to_tokens(text):

  docs = {}

  i = 0

  for doc in text:
    docs[i] = doc.split()

    i = i + 1

  splited_docs_val = docs.values()

  return splited_docs_val


# Function(2) : return the the docs that have the same token with query

def num_of_doc_of_same_query(spli):

 lis = []

 for i in range(len(splited_query)):

   doc_num = 0

   for j in spli:

     doc_num = doc_num + 1

     for k in range(len(j)):

       if splited_query[i] == j[k]:
         lis.append(doc_num)

 return lis


# Function(3) : the all function

def docs_IDs(text):
  split_docs = split_doc_to_tokens(text)

  nums = num_of_doc_of_same_query(split_docs)

  return nums


def Retrieve_docs(dfs , text):
  split_doc = split_doc_to_tokens(text)

  numo = num_of_doc_of_same_query(split_doc)

  for d in numo:
    # Printing value of column 'A' at index 2
    print(f"Document Number {d}: \n {dfs['processed_text'].iloc[d]}")



splited_docs_values = docs_IDs(df['processed_text'])

print(splited_docs_values)

# Rank the retrievd documents based ranking algorithm (TF-IDF)

tfidf_retr = pt.BatchRetrieve(index, controls = {"wmodel": "TF_IDF"})

results = tfidf_retr.search(query)
  results

"""Query Expansion"""

import pandas as pd
import pyterrier as pt
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import re
import os
pd.set_option('display.max_colwidth', 150)

# Commented out IPython magic to ensure Python compatibility.
# Need to install additional terrier package for PRF. It will take around 1 min
!git clone https://github.com/terrierteam/terrier-prf/
!apt-get install maven   #used for Java projects to manage project dependencies and build processes
# %cd /content/terrier-prf/
!mvn install
!pwd
# %cd ..

# Define our retrieval model
bm25 = pt.BatchRetrieve(index, wmodel="BM25",num_results=10)

result = bm25.search(query)
result

df[['text']][df['docno'].isin(results['docno'].loc[0:4].tolist())]

# "rewrite" function from PyTerrier will be used to expand queries specifying RM3 as the model
# fb_docs ==> no. expansion documents
# fb_terms ==> no. expansion terms
rm3_expander = pt.rewrite.RM3(index,fb_terms=10, fb_docs=100)

#output of the BM25 will be fed into the RM3 expander for query expansion.
rm3_qe = bm25 >> rm3_expander
expanded_query = rm3_qe.search(query).iloc[0]["query"]

expanded_query

# Just print the expanded query with term scores
for s in expanded_query.split()[1:]:
  print(s)

print("\n" + query)

# After that you can search using the expanded query
expanded_query_formatted = ' '.join(expanded_query.split()[1:])

results_wqe = bm25.search(expanded_query_formatted)

print("   Before Expansion    After Expansion")
print(pd.concat([results[['docid','score']][0:5].add_suffix('_1'),
            results_wqe[['docid','score']][0:5].add_suffix('_2')], axis=1).fillna(''))

#Let's check the tweets text for the top 5 retrieved tweets
df[['text']][df['docno'].isin(results_wqe['docno'].loc[0:5].tolist())]

import tensorflow as tf
import tensorflow_hub as hub
import numpy as np

#load the ELMo model
elmo = hub.load("https://tfhub.dev/google/elmo/3")

#define the sentences
queryyy1 = "Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality"
queryyy1 = preprocess(queryyy1)
queryyy2 = "Preclinical studies have shown that statins, particularly simvastatin, can prevent growth in breast cancer cell lines and animal model"
queryyy2 = preprocess(queryyy2)
sentences = [queryyy1, queryyy2]

#generate ELMo embeddings for the sentences
embeddings = elmo.signatures["default"](tf.constant(sentences))["elmo"]


studies_queryyy1_embedding = embeddings.numpy()[0][1]
studies_queryyy2_embedding = embeddings.numpy()[1][1]

#print the embeddings vectors
print("Embedding vector for 'studies' (queryyy1):", studies_queryyy1_embedding)
print("Embedding vector for 'studies' (queryyy2):", studies_queryyy2_embedding)

embeddings.numpy().shape

"""UI"""

!pip install flask_ngrok

df2 = df.head(50)

df2 = df2.to_dict()

df2

def func(df2 , que):
 i = 0

 quer = preprocess(que)

 docs_id = []

 for key, value in df2.items():
   if key == 'processed_text':
         val = value.values()
         for doc in val:
           terms = doc.split()
           for term in terms:
             if term == quer and i not in docs_id:
               docs_id.append(f'''Document number {i} -----> \n{df["text"][i]}''')
           i = i + 1
 return docs_id

query2 = "study"

x = func(df2 , query2)
x

from google.colab.output import eval_js
print (eval_js("google.colab.kernel.proxyPort(5000)"))

from flask import Flask, request
from flask_ngrok import run_with_ngrok

# Assuming you've already defined the sui function and imported necessary modules

app = Flask(__name__)
run_with_ngrok(app)

@app.route("/")
def home():
    return """
    <style>
        body {
            background-color: white;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
        }

        .header {
            background-color: black;
            color: white;
            padding: 20px 0;
        }

        .container {
            text-align: center;
        }

        h1 {
            text-align: center;
            margin: 0;
            padding: 10px 0;
        }

        #searchInput {
            padding: 10px;
            border: 1px solid #ccc;
            border-radius: 20px; /* Increased border-radius for a rounded appearance */
            margin-bottom: 10px;
            width: 300px; /* Adjust the width as needed */
            box-sizing: border-box; /* Include padding and border in the element's total width */
            transition: border-color 0.3s; /* Smooth transition for border color change */
        }

        #searchInput:focus {
            border-color: #007bff; /* Change border color on focus */
        }

        button {
            padding: 10px 20px;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 20px; /* Increased border-radius for a rounded appearance */
            cursor: pointer;
            transition: background-color 0.3s; /* Smooth transition for background color change */
        }

        button:hover {
            background-color: #0056b3; /* Change background color on hover */
        }
    </style>

    <div class="header">
        <h1>Welcome to Kareem's Search Engine</h1>
    </div>
    <div class="container">
        <input type="text" id="searchInput" placeholder="Enter your query...">
        <button onclick="search()">Search</button>
    </div>
    <div id="searchResult"></div>

    <script>
        function search() {
            var searchTerm = document.getElementById("searchInput").value;
            fetch('/search', {
                method: 'POST',
                body: JSON.stringify({ query: searchTerm }),
                headers:{
                    'Content-Type': 'application/json'
                }
            })
            .then(response => response.json())
            .then(data => {
                console.log("Received data:", data); // Debug: Check if data is received
                var resultDiv = document.getElementById("searchResult");
                resultDiv.innerHTML = "<h2>Relevant Documents IDs:</h2>";
                if (data.results.length === 0) {
                    resultDiv.innerHTML += "<p>No documents found</p>";
                } else {
                    data.results.forEach(doc => {
                        console.log("Displaying document:", doc); // Debug: Check if document is displayed
                        resultDiv.innerHTML += "<p>" + doc + "</p>";
                    });
                }
            })
            .catch(error => {
                console.error('Error occurred during fetch:', error); // Debug: Log fetch errors
            });
        }
    </script>
    """

@app.route("/search", methods=['POST'])
def search():
    query = request.json['query']
    print("Received query:", query)  # Debug: Check if Flask receives the query
    results = func(df2, query)
    print("Search results:", results)  # Debug: Check if sui function returns results
    return {'results': results}

app.run()

"""Evaluation"""

import pandas as pd
import pyterrier as pt
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import re
import os
pd.set_option('display.max_colwidth', 150)

vaswani_dataset = pt.datasets.get_dataset("vaswani")

dff = vaswani_dataset.get_topics()

dff['docno'] = dff.index

# Rename column 'A' to 'X'
dff = dff.rename(columns={'query': 'Text'})

qrels = vaswani_dataset.get_qrels()

qrels['docno']=qrels['docno'].astype(str)

dff

indexref2 = vaswani_dataset.get_index()
index2 = pt.IndexFactory.of(indexref2)

print(index2.getCollectionStatistics().toString())

# qrels = pd.read_csv("/mnt/data/queries.csv")

retr = pt.BatchRetrieve(index2, controls = {"wmodel": "TF_IDF"})

res = retr.search("mathematical")
res

eval = pt.Evaluate(res,qrels)
eval